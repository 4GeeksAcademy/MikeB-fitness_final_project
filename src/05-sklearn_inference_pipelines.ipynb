{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8118c4d9",
   "metadata": {},
   "source": [
    "# Putting it all together: SciKit-Learn inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0380a",
   "metadata": {},
   "source": [
    "## 1. Define a column transformer that encodes the categorical features\n",
    "\n",
    "See SciKit-Learn [`ColumnTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd455033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1af14a",
   "metadata": {},
   "source": [
    "## 2. Define a pipeline that takes raw input and does prediction\n",
    "\n",
    "See SciKit-Learn [`Pipeline()`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) documentation.\n",
    "\n",
    "The pipeline will have three steps:\n",
    "\n",
    "1. **Feature encoder** - (column transformer from section 1, above) - use the strategy you devised in the data preparation notebook.\n",
    "2. **Imputer** - to fill in any features the user dident know/specify - use the strategy you came up with in the user input imputation notebook.\n",
    "3. **Regressor** - use the optimized hyperparameters from the model building notebook.\n",
    "\n",
    "Make TWO pipelines, one for the time model and one for the calories model, and store them in a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5020ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines={}\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af953b",
   "metadata": {},
   "source": [
    "## 3. Train the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76becb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here... Be sure to train on NaN salted data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0d60a",
   "metadata": {},
   "source": [
    "## 4. Evaluate the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc84f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here... Use cross-validation and/or predictions on the test set. Results should be close to those in the model building notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c953d2c",
   "metadata": {},
   "source": [
    "## 5. Save pipeline for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0161c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('../models').mkdir(exist_ok=True)\n",
    "\n",
    "with open('../models/pipelines.pkl', 'wb') as output_file:\n",
    "    pickle.dump(pipelines, output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
