{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b118a39",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03c634",
   "metadata": {},
   "source": [
    "## 1. Asset loading\n",
    "\n",
    "### 1.1. Feature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/biometric_features.pkl', 'rb') as input_file:\n",
    "    biometric_features=pickle.load(input_file)\n",
    "\n",
    "with open('../data/input_features.pkl', 'rb') as input_file:\n",
    "    input_features=pickle.load(input_file)\n",
    "\n",
    "with open('../data/output_features.pkl', 'rb') as input_file:\n",
    "    output_features=pickle.load(input_file)\n",
    "\n",
    "with open('../data/categorical_features.pkl', 'rb') as input_file:\n",
    "    categorical_features=pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb8b1a",
   "metadata": {},
   "source": [
    "### 1.2. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train.pkl', 'rb') as input_file:\n",
    "    train_df=pickle.load(input_file)\n",
    "\n",
    "with open('../data/processed/test.pkl', 'rb') as input_file:\n",
    "    train_df=pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112fbbc",
   "metadata": {},
   "source": [
    "## 2. Model Building\n",
    "\n",
    "We need to build two models - one to predict time and the other to predict calories. We will again use a dictionary to keep things organized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae144d",
   "metadata": {},
   "source": [
    "### 2.1. Model dictionary definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ff33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\n",
    "    'calorie_model': HistGradientBoostingRegressor(early_stopping=True),\n",
    "    'time_model': HistGradientBoostingRegressor(early_stopping=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461bb83",
   "metadata": {},
   "source": [
    "### 2.2. Naive model cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b311be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results={}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "\n",
    "    # Your code here... Remember: the features names for each model are the biometric\n",
    "    # features + the 'extra' feature from the input features dictionary we\n",
    "    # loaded above. The label name is also stored in the output features dictionary.\n",
    "    # The keys match across the models, input features and output features dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cd8f8",
   "metadata": {},
   "source": [
    "## 3. Model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    'max_iter': randint(10, 10000)\n",
    "    # etc.\n",
    "}\n",
    "optimized_hyperparameters={}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "\n",
    "    # Your code here... HistGradientBoostingRegressor is fast on this dataset, so use\n",
    "    # RandomizedSearchCV with a few hundred or thousand iterations. Make sure to replace\n",
    "    # the naive model with the optimized one in the models dictionary and store the\n",
    "    # winning hyperparameters using the model name as key in the optimized hyperparameters\n",
    "    # dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df714f80",
   "metadata": {},
   "source": [
    "## 4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12367b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here... Make predictions on the test set with the optimized time and calorie models.\n",
    "# Then evaluate those predictions - plotting predicted vs true values and/or fit residuals is\n",
    "# a good idea, you also probably want to look at the RMSE between predictions and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f4273",
   "metadata": {},
   "source": [
    "## 5. Save assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e607da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('../models').mkdir(exist_ok=True)\n",
    "\n",
    "with open('../models/optimized_models.pkl', 'wb') as output_file:\n",
    "    pickle.dump(models, output_file)\n",
    "\n",
    "with open('../models/optimized_hyperparameters.pkl', 'wb') as output_file:\n",
    "    pickle.dump(optimized_hyperparameters, output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
